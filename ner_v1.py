# -*- coding: utf-8 -*-
"""NER_V1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/156msAHFlTFon7eka2Hncwu_2jM5Xkygh

# **STEP 1 : Transforming Dataset from text to json format with : text+ labels and their locations**
"""

# Import the various libraries
import re, json, os, itertools
import pandas as pd 
from tqdm import tqdm

# Path to each individual txt files converted from PDF
TC_PATH = "/content/drive/MyDrive/CUAD-v1/full_contract_txt/"


# Path to folder containing all the CUAD data and files
MASTER_PATH = "/content/drive/MyDrive/CUAD-v1/"

# Name of CSV file containing all the extracted clauses from the Atticus team
MASTER_CLAUSES = 'master_clauses.csv'

# Name of JSON file to export the agreement text and labels for data extraction
JSON_EXPORT = '/content/drive/MyDrive/paper-2-data-main/jsonl_cuadv1.json'

# Name of JSON file to export the agreement taxt and labels for further inspection
JSON_EXPORT_INSPECT = '/content/drive/MyDrive/paper-2-data-main/jsonl_cuadv1_inspect.json'

# Walk through all .txt filenames and create a dataframe with the names of the files, sorted alpha/num
text_files = []
for (dirpath, dirnames, filenames) in os.walk(TC_PATH):
    text_files.extend(filenames)

tf_df = pd.DataFrame(data = text_files, columns = ['Text Files'])
tf_df.sort_values('Text Files', axis=0, inplace=True, ignore_index=True) 
tf_df.head()

# Read master clauses CSV into a dataframe, sort by filename to match text file dataframe created above
mc_df = pd.read_csv("/content/drive/MyDrive/CUAD-v1/master_clauses.csv")

# Sort the dataframe by filename
mc_df.sort_values('Filename', axis=0, inplace=True, ignore_index=True) 
# Bring in the list of the .txt filenames
mc_df['Text Files']=tf_df
# Create a list of the names of the files, with index num
file_list = [(index, row['Text Files']) for index, row in mc_df.iterrows()]

# Create a function to clean up and pre-process the text.
# This process should be used for any document text inc. train, validation and test sets.
def pre_process_doc_common(text):
    # Simple replacement for "\n"
    text = text.replace("\n", " ")     
    
    # Simple replacement for "\xa0"
    text = text.replace("\xa0", " ")  
    
    # Simple replacement for "\x0c"
    text = text.replace("\x0c", " ")
    
    # Get rid of multiple dots
    regex = "\ \.\ "
    subst = "."
    text = re.sub(regex, subst, text, 0)
    
    # Get rid of underscores
    regex = "_"
    subst = " "
    text = re.sub(regex, subst, text, 0)
    
    # Get rid of multiple dashes
    regex = "--+"
    subst = " "
    text = re.sub(regex, subst, text, 0)
    
    # Get rid of multiple stars
    regex = "\*+"
    subst = "*"
    text = re.sub(regex, subst, text, 0)
    
    # Get rid of multiple whitespace
    regex = "\ +"
    subst = " "
    text = re.sub(regex, subst, text, 0)
    
    #Strip leading and trailing whitespace
    text = text.strip()
    
    return text

# Function to take in the file list, read each file, clean the text and return all agreements in a list
def text_data(file_list, print_text=False, clean_text=True, max_len=3000):
    text_list = []
    for index, filename in tqdm(file_list):
        agreement = open(TC_PATH+filename, "r", encoding="utf8")
        text = agreement.read()
        if print_text:
            print("Text before cleaning: \n", text)
        
        # Run text through cleansing function
        if clean_text:
            text = pre_process_doc_common(text)
        text = text[:max_len]
        len_text = len(text)
        
        if print_text:
            print("Text after cleaning: \n", text)
        
        text_list.append([index,
                  filename,
                  text,
                  len_text])
        
    return text_list

# Clean text and create dataframe with the text of ech document
data = text_data(file_list, print_text=False, clean_text=True, max_len=3000)
columns = ['ID', 'Documents', 'Text', 'Length_Of_Text']
text_df = pd.DataFrame(data=data, columns=columns)

# Add the two columns to a copy of the main dataframe
#mc_df_wk = mc_df_cut.copy()
mc_df_wk = mc_df.copy()
mc_df_wk = mc_df_wk.join(text_df[['Text', 'Length_Of_Text']])

#Ensure agreement date, doc_name and parties are list objects
mc_df_wk["Agreement Date"] = mc_df_wk["Agreement Date"].apply(eval)
mc_df_wk["Document Name"] = mc_df_wk["Document Name"].apply(eval)
mc_df_wk["Parties"] = mc_df_wk["Parties"].apply(eval)
mc_df_wk["Effective Date"] = mc_df_wk["Effective Date"].apply(eval)
mc_df_wk["Expiration Date"] = mc_df_wk["Expiration Date"].apply(eval)
mc_df_wk["Renewal Term"] = mc_df_wk["Renewal Term"].apply(eval)
mc_df_wk["Notice Period To Terminate Renewal"] = mc_df_wk["Notice Period To Terminate Renewal"].apply(eval)
mc_df_wk["Governing Law"] = mc_df_wk["Governing Law"].apply(eval)
mc_df_wk["Most Favored Nation"] = mc_df_wk["Most Favored Nation"].apply(eval)
mc_df_wk["Competitive Restriction Exception"] = mc_df_wk["Competitive Restriction Exception"].apply(eval)
mc_df_wk["Non-Compete"] = mc_df_wk["Non-Compete"].apply(eval)
mc_df_wk["Exclusivity"] = mc_df_wk["Exclusivity"].apply(eval)
mc_df_wk["No-Solicit Of Customers"] = mc_df_wk["No-Solicit Of Customers"].apply(eval)
mc_df_wk["No-Solicit Of Employees"] = mc_df_wk["No-Solicit Of Employees"].apply(eval)
mc_df_wk["Non-Disparagement"] = mc_df_wk["Non-Disparagement"].apply(eval)
mc_df_wk["Termination For Convenience"] = mc_df_wk["Termination For Convenience"].apply(eval)
mc_df_wk["Rofr/Rofo/Rofn"] = mc_df_wk["Rofr/Rofo/Rofn"].apply(eval)
mc_df_wk["Change Of Control"] = mc_df_wk["Change Of Control"].apply(eval)
mc_df_wk["Anti-Assignment"] = mc_df_wk["Anti-Assignment"].apply(eval)
mc_df_wk["Revenue/Profit Sharing"] = mc_df_wk["Revenue/Profit Sharing"].apply(eval)
mc_df_wk["Price Restrictions"] = mc_df_wk["Price Restrictions"].apply(eval)
mc_df_wk["Minimum Commitment"] = mc_df_wk["Minimum Commitment"].apply(eval)
mc_df_wk["Volume Restriction"] = mc_df_wk["Volume Restriction"].apply(eval)
mc_df_wk["Ip Ownership Assignment"] = mc_df_wk["Ip Ownership Assignment"].apply(eval)
mc_df_wk["Joint Ip Ownership"] = mc_df_wk["Joint Ip Ownership"].apply(eval)
mc_df_wk["License Grant"] = mc_df_wk["License Grant"].apply(eval)
mc_df_wk["Non-Transferable License"] = mc_df_wk["Non-Transferable License"].apply(eval)
mc_df_wk["Affiliate License-Licensor"] = mc_df_wk["Affiliate License-Licensor"].apply(eval)
mc_df_wk["Affiliate License-Licensee"] = mc_df_wk["Affiliate License-Licensee"].apply(eval)
mc_df_wk["Unlimited/All-You-Can-Eat-License"] = mc_df_wk["Unlimited/All-You-Can-Eat-License"].apply(eval)
mc_df_wk["Irrevocable Or Perpetual License"] = mc_df_wk["Irrevocable Or Perpetual License"].apply(eval)
mc_df_wk["Source Code Escrow"] = mc_df_wk["Source Code Escrow"].apply(eval)
mc_df_wk["Post-Termination Services"] = mc_df_wk["Post-Termination Services"].apply(eval)
mc_df_wk["Audit Rights"] = mc_df_wk["Audit Rights"].apply(eval)
mc_df_wk["Uncapped Liability"] = mc_df_wk["Uncapped Liability"].apply(eval)
mc_df_wk["Cap On Liability"] = mc_df_wk["Cap On Liability"].apply(eval)
mc_df_wk["Liquidated Damages"] = mc_df_wk["Liquidated Damages"].apply(eval)
mc_df_wk["Warranty Duration"] = mc_df_wk["Warranty Duration"].apply(eval)
mc_df_wk["Insurance"] = mc_df_wk["Insurance"].apply(eval)
mc_df_wk["Covenant Not To Sue"] = mc_df_wk["Covenant Not To Sue"].apply(eval)
mc_df_wk["Third Party Beneficiary"] = mc_df_wk["Third Party Beneficiary"].apply(eval)

# Some document name references have more than one entry - remove them for further inspection later
mc_df_wk['Doc_N_Length'] = mc_df_wk['Document Name'].str.len()
mc_df_mul = mc_df_wk[mc_df_wk.Doc_N_Length > 1]
mc_df_wk.drop(mc_df_mul.index, inplace=True)

# Have a look at the data
mc_df_wk.head(3)

# Agreement date is an important label. Here we will drop any agreement without a date.
# These will typically be template or specimen agreements which havent been executed
# Prior to dropping, we create a dataframe to manually check and annotate agreement date in a different exercise
mc_df_nul = mc_df_wk[mc_df_wk["Agreement Date-Answer"].isnull()]
mc_df_wk = mc_df_wk.dropna(subset=['Agreement Date-Answer'])
mc_df_wk.head()

# The CUADv1 labels includes the Party definition eg Apple Inc. "Apple", here we keep just the legal entity:
def remove_party_overlaps(labels):
    labels.sort()
    k = []
    for i in range(len(labels)-1):
        l1 = labels[i]
        l2 = labels[i+1]
        if l1[0] == l2[0]:
            len1 = l1[1] - l1[0]
            len2 = l2[1] - l2[0]
            if len1 > len2:
                k.append(l1)
                continue
            else:
                k.append(l2)
                continue
        else:
            k.append(labels[i])
    new_labels = list(k for k,_ in itertools.groupby(k))
    
    return new_labels

# Go through each label and find the label in the text, ensure label is pre-processed same as text.
# If labels don't match, append to a seperate file to check.

clean_text = True
djson = list()
djson_inspect = list()
for index, row in tqdm(mc_df_wk.iterrows()):
    labels = list()
    ids = index
    text = row['Text']
  
    #DOC_NAME
    doc_names = row['Document Name']
    for name in doc_names:
        if clean_text:
            name = pre_process_doc_common(name)
        matches = re.finditer(re.escape(name.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'DOC_NAME'])
    
    #AGMT_DATE
    agmt_date = row['Agreement Date']
    for date in agmt_date:
        if clean_text:
            date = pre_process_doc_common(date)
        matches = re.finditer(re.escape(date.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'AGMT_DATE'])

    #PARTIES
    parties = row['Parties']
    for party in parties:
        if clean_text:
            party = pre_process_doc_common(party)
        matches = re.finditer(re.escape(party.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'PARTY'])
        
    #Effective Date
    effective_date = row['Effective Date']
    for e_date in effective_date:
        if clean_text:
            e_date = pre_process_doc_common(e_date)
        matches = re.finditer(re.escape(e_date.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'EFFECTIVE_DATE'])
            
    #Expiration Date
    expiration_date = row['Expiration Date']
    for ex_date in expiration_date:
        if clean_text:
            ex_date = pre_process_doc_common(ex_date)
        matches = re.finditer(re.escape(ex_date.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'EXPIRATION_DATE'])
                 
    #Renewal Term
    term = row['Renewal Term']
    for t in term:
        if clean_text:
            t = pre_process_doc_common(t)
        matches = re.finditer(re.escape(t.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'RENEWAL_TERM'])   
    
    #Notice Period To Terminate Renewal
    terminate_Renewal = row['Notice Period To Terminate Renewal']
    for terminate in terminate_Renewal:
        if clean_text:
            terminate = pre_process_doc_common(terminate)
        matches = re.finditer(re.escape(terminate.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'NOTICE_PERIOD_TERMINATE_RENEWAL'])   
    
    #Governing Law
    gov_law = row['Governing Law']
    for law in gov_law:
        if clean_text:
            law = pre_process_doc_common(law)
        matches = re.finditer(re.escape(law.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'GOVERNING_LAW'])   
            
    #Most Favored Nation
    favored_nation = row['Most Favored Nation']
    for nation in favored_nation:
        if clean_text:
            nation = pre_process_doc_common(nation)
        matches = re.finditer(re.escape(nation.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'MOST_FAVORED_NATION'])   
            
    #Competitive Restriction Exception
    restr_exception = row['Competitive Restriction Exception']
    for exception in restr_exception:
        if clean_text:
            exception = pre_process_doc_common(exception)
        matches = re.finditer(re.escape(exception.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'COMPETITIVE_RESTRICTION_EXCEPTION'])   
            
    #Non-Compete
    non_compete = row['Non-Compete']
    for n_compete in non_compete:
        if clean_text:
            n_compete = pre_process_doc_common(n_compete)
        matches = re.finditer(re.escape(n_compete.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'NON_COMPETE'])   
            
    #Exclusivity
    exclusivity = row['Exclusivity']
    for exclusi in exclusivity:
        if clean_text:
            exclusi = pre_process_doc_common(exclusi)
        matches = re.finditer(re.escape(exclusi.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'EXCLUSIVITY'])   
            
            
    #No-Solicit Of Customers
    no_solicit_customers= row['No-Solicit Of Customers']
    for customers in no_solicit_customers:
        if clean_text:
            customers = pre_process_doc_common(customers)
        matches = re.finditer(re.escape(customers.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'NO_SOLICIT_OF_CUSTOMERS'])   
            
    #No-Solicit Of Employees
    no_solicit_employees = row['No-Solicit Of Employees']
    for employees in no_solicit_employees:
        if clean_text:
            employees = pre_process_doc_common(employees)
        matches = re.finditer(re.escape(employees.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'NO_SOLICIT_OF_EMPLOYEES'])   
            
    #Non-Disparagement
    non_disparagement = row['Non-Disparagement']
    for disparagement in non_disparagement:
        if clean_text:
            disparagement = pre_process_doc_common(disparagement)
        matches = re.finditer(re.escape(disparagement.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'NON_DISPARAGEMENT'])   
            
            
    #Termination For Convenience
    termination_convenience = row['Termination For Convenience']
    for tc in termination_convenience:
        if clean_text:
            tc = pre_process_doc_common(tc)
        matches = re.finditer(re.escape(tc.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'TERMINATION_FOR_CONVENIENCE'])   
            
    #Rofr/Rofo/Rofn
    rofr = row['Rofr/Rofo/Rofn']
    for r in rofr:
        if clean_text:
            r = pre_process_doc_common(r)
        matches = re.finditer(re.escape(r.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'ROFR/ROFO/ROFN'])   
            
    #Change Of Control
    change_control = row['Change Of Control']
    for cc in change_control:
        if clean_text:
            cc = pre_process_doc_common(cc)
        matches = re.finditer(re.escape(cc.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'CHANGE_OF_CONTROL'])   
            
    
    #Anti-Assignment
    anti_assignment= row['Anti-Assignment']
    for aa in anti_assignment:
        if clean_text:
            aa = pre_process_doc_common(aa)
        matches = re.finditer(re.escape(aa.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'ANTI_ASSIGNMENT'])   
            
    #Revenue/Profit Sharing
    revenue = row['Revenue/Profit Sharing']
    for rev in revenue:
        if clean_text:
            rev = pre_process_doc_common(rev)
        matches = re.finditer(re.escape(rev.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'REVENUE/PROFIT_SHARING'])  
            
    #Price Restrictions
    price= row['Price Restrictions']
    for pr in price:
        if clean_text:
            pr = pre_process_doc_common(pr)
        matches = re.finditer(re.escape(pr.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'PRICE_RESTRICTION'])  
            
    #Minimum Commitment
    min_commit= row['Minimum Commitment']
    for mi in min_commit:
        if clean_text:
            mi = pre_process_doc_common(mi)
        matches = re.finditer(re.escape(mi.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'MINIMUM_COMMITMENT'])  
            
    #Volume Restriction
    volume= row['Volume Restriction']
    for vol in volume:
        if clean_text:
            vol = pre_process_doc_common(vol)
        matches = re.finditer(re.escape(vol.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'VOLUME_RESTRICTION'])  
            
    #Ip Ownership Assignment
    ip_ownership= row['Ip Ownership Assignment']
    for ip in ip_ownership:
        if clean_text:
            ip = pre_process_doc_common(ip)
        matches = re.finditer(re.escape(ip.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'IP_OWNERSHIP_ASSIGNMENT'])  
            
    #Joint Ip Ownership
    joint_ip= row['Joint Ip Ownership']
    for joi in joint_ip:
        if clean_text:
            joi = pre_process_doc_common(joi)
        matches = re.finditer(re.escape(joi.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'JOINT_IP_OWNERSHIP'])  
            
    #Anti-Assignment
    anti_assignment= row['Anti-Assignment']
    for aa in anti_assignment:
        if clean_text:
            aa = pre_process_doc_common(aa)
        matches = re.finditer(re.escape(aa.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'ANTI_ASSIGNMENT'])  
            
    #License Grant
    grant= row['License Grant']
    for gr in grant:
        if clean_text:
            gr = pre_process_doc_common(gr)
        matches = re.finditer(re.escape(gr.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'LICENSE_GRANT'])  
            
    #Non-Transferable License
    transfer_license= row['Non-Transferable License']
    for tran in transfer_license:
        if clean_text:
            tran = pre_process_doc_common(tran)
        matches = re.finditer(re.escape(tran.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'NON_TRANSFERABLE_LICENSE'])  
    
    #Affiliate License-Licensor
    affiliate_license= row['Affiliate License-Licensor']
    for aff in affiliate_license:
        if clean_text:
            aff = pre_process_doc_common(aff)
        matches = re.finditer(re.escape(aff.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'AFFILIATE_LICENSE_LICENSOR'])
            
    #Affiliate License-Licensee
    affi_licensee= row['Affiliate License-Licensee']
    for licen in affi_licensee:
        if clean_text:
            licen = pre_process_doc_common(licen)
        matches = re.finditer(re.escape(licen.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'AFFILIATE_LICENSE_LICENSEE'])
            
    #Unlimited/All-You-Can-Eat-License
    unlimited= row['Unlimited/All-You-Can-Eat-License']
    for unlimit in unlimited:
        if clean_text:
            unlimit = pre_process_doc_common(unlimit)
        matches = re.finditer(re.escape(unlimit.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'UNLIMITED/ALL_YOU_CAN_EAT_LICNESE'])
    
    #Irrevocable Or Perpetual License
    irrevocable= row['Irrevocable Or Perpetual License']
    for irr in irrevocable:
        if clean_text:
            irr = pre_process_doc_common(irr)
        matches = re.finditer(re.escape(irr.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'IRREVOCABLE_OR_PERPETUAL_LICENSE'])
            
    #Source Code Escrow
    source= row['Source Code Escrow']
    for so in source:
        if clean_text:
            so = pre_process_doc_common(so)
        matches = re.finditer(re.escape(so.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'SOURCE_CODE_ESCROW'])
            
    #Post-Termination Services
    post= row['Post-Termination Services']
    for po in post:
        if clean_text:
            po = pre_process_doc_common(po)
        matches = re.finditer(re.escape(po.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'POST_TERMINATION_SERVICES'])
            
    #Audit Rights
    audit= row['Audit Rights']
    for aud in audit:
        if clean_text:
            aud = pre_process_doc_common(aud)
        matches = re.finditer(re.escape(aud.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'AUDIT_RIGHTS'])
            
    #Uncapped Liability
    uncapped= row['Uncapped Liability']
    for uncap in uncapped:
        if clean_text:
            uncap = pre_process_doc_common(uncap)
        matches = re.finditer(re.escape(uncap.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'UNCAPPED_LIABILITY'])
            
    #Cap On Liability
    cap_liability= row['Cap On Liability']
    for cap in cap_liability:
        if clean_text:
            cap = pre_process_doc_common(cap)
        matches = re.finditer(re.escape(cap.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'CAP_ON_LIABILITY'])
            
    #Liquidated Damages
    damages= row['Liquidated Damages']
    for da in damages:
        if clean_text:
            da = pre_process_doc_common(da)
        matches = re.finditer(re.escape(da.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'LIQUIDATED_DAMAGES'])
            
    #Warranty Duration
    duration= row['Warranty Duration']
    for dur in duration:
        if clean_text:
            dur = pre_process_doc_common(dur)
        matches = re.finditer(re.escape(dur.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'WARRANTY_DURATION'])
    
    #Insurance
    insurance= row['Insurance']
    for ins in insurance:
        if clean_text:
            ins = pre_process_doc_common(ins)
        matches = re.finditer(re.escape(ins.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'INSURANCE'])
    #Covenant Not To Sue
    convenant= row['Covenant Not To Sue']
    for conv in convenant:
        if clean_text:
            conv = pre_process_doc_common(conv)
        matches = re.finditer(re.escape(conv.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'CONVENANT NOT TO SUE'])
            
    #Third Party Beneficiary
    beneficiary= row['Third Party Beneficiary']
    for ben in beneficiary:
        if clean_text:
            ben = pre_process_doc_common(ben)
        matches = re.finditer(re.escape(ben.lower()), text.lower())
        for m in matches:
            s = m.start()
            e = m.end()
            labels.append([s, e, 'THIRD_PARTY_BENEFICIARY'])
     
    
    labels = remove_party_overlaps(labels)
    #print(labels)
    djson_inspect.append({'id': ids, 'text': text, "label": labels})


# Add to the check JSON file the other documents excluded due to duplicate names and no agreement dates

for index, row in tqdm(mc_df_mul.iterrows()):
    labels = list()
    ids = index
    text = row['Text']
    djson_inspect.append({'id': ids, 'text': text, "label": labels})

for index, row in tqdm(mc_df_nul.iterrows()):
    labels = list()
    ids = index
    text = row['Text']
    djson_inspect.append({'id': ids, 'text': text, "label": labels})

# Export the full datasets for import to Doccano

filepath = JSON_EXPORT
open(filepath, 'w').write("\n".join([json.dumps(e) for e in djson]))

filepath = JSON_EXPORT_INSPECT
open(filepath, 'w').write("\n".join([json.dumps(e) for e in djson_inspect]))

"""# *STEP 2 :Labeling Our Dataset & Train it *"""

!nvcc --version

!pip install --upgrade spacy

!pip install --upgrade spacy[cuda111,transformers]

!pip install jsonlines
!python -m spacy download en_core_web_sm

'''
import spacy 
print(spacy.__version__)

'''
import jsonlines

from tqdm.autonotebook import tqdm
import jsonlines
import re

import spacy
from spacy import displacy
assert spacy.__version__ == "3.3.0"
from spacy.training import offsets_to_biluo_tags # requires spaCy 3.0

import pandas as pd

nlp= spacy.load("en_core_web_sm")

'''
with jsonlines.open(JSON_EXPORT_INSPECT, "r") as f:
    articles = list(f.iter())

'''
articles=pd.read_json (JSON_EXPORT_INSPECT, lines=True)
articles.head()

#article=articles[7]
#doc = nlp(article['text'])
articles['tokens'] = articles['text'].apply(lambda x: nlp(x))
articles.head()

row = articles.iloc[5]
doc = row['tokens']
spans = row['label']

ents = []
for span_start, span_end, label in spans:
    ent = doc.char_span(span_start, span_end, label=label)
    if ent is None:
        continue

    ents.append(ent)

doc.ents = ents
#doc.spans['index'] = list(ents)
#ents = list(doc.spans['index'])


#doc.ents
displacy.render(doc, style="ent", jupyter=True)

# Each word must be seperated for the transformer using the IOB format
# Create tags using token.ent_iob_ and add to the DataFrame
# Allow for any character misalignment between spaCy tokenization and Doccano character indices
tags_list_iob = []
for index, row in articles.iterrows():
  doc = row['tokens']
  ents = []

  for span_start, span_end, label in row['label']:
    ent = doc.char_span(span_start, span_end, label=label)
    if ent is None:
      continue
    ents.append(ent)

  #doc.ents =list(ents)
  doc.spans['index'] = list(ents)

  iob_tags = [f"{t.ent_iob_}-{t.ent_type_}" if t.ent_iob_ != "O" else "O" for t in doc]
  tags_list_iob.append(iob_tags)
articles['tags'] = tags_list_iob

# Generate list of the IOB feature class labels from tags
all_tags = list(itertools.chain.from_iterable(tags_list_iob))

def unique(list1):
    # insert the list to the set
    list_set = set(list1)
    # convert the set to the list
    unique_list = (list(list_set))
    unique_list.sort()
    return unique_list

feature_class_labels = unique(all_tags)
print(feature_class_labels)

articles.head()

just_text =articles['text']
docs = list(tqdm(nlp.pipe(just_text), total=len(just_text)))

# Generate the NER index tags for each token
articles['ner_tags'] = articles['tags'].apply(lambda x: [feature_class_labels.index(tag) for tag in x])

# Split tokens into a list ready for CSV
articles['split_tokens'] = articles['tokens'].apply(lambda x: [tok.text for tok in x])

# Check dataframe head
articles.head()

articles.iloc[7]

FEATURE_CLASS_LABELS = "/content/drive/MyDrive/paper-2-data-main/feature_class_labels.json"
# Generate list of the IOB feature class labels from tags
lab=[(i.ent_iob_ + "-" + i.ent_type_) for i in doc]
# Generate list of the IOB feature class labels from tags
def unique(list1):
    # insert the list to the set
    list_set = set(list1)
    # convert the set to the list
    unique_list = (list(list_set))
    unique_list.sort()
    return unique_list

feature_class_labels = unique(lab)

# Export Feature Class Labels for use in Transformer fine tuning
with open(FEATURE_CLASS_LABELS, 'w') as f:
    json.dump(feature_class_labels, f, indent=2)

# Open the label list created in pre-processing corresponding to the ner_tag indices
with open(FEATURE_CLASS_LABELS, 'r') as f:
    label_list = json.load(f)

for n in range(len(label_list)):
    print(n, label_list[n])

from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline

model_name = "deepset/roberta-base-squad2"

hugg = pipeline('question-answering', model=model_name, tokenizer=model_name)

QA_input = {
    'question': " effective date ?",
    'context':articles[1]['text']
}
res = hugg(QA_input)

print(res)

"""#**STEP 3 : Deberta model**"""

!pip install -U torch

!pip install -U transformers

!pip install -U wandb

pip install datasets

import os, re, math, random, json, string

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import display, HTML
import wandb

import transformers
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer
from transformers import TrainerCallback, AdamW, get_cosine_schedule_with_warmup
from transformers import DataCollatorForTokenClassification, PreTrainedModel, RobertaTokenizerFast

from datasets import load_dataset, ClassLabel, Sequence, load_metric

# Need to log in to weights and biases in the command line using: wandb login
wandb.login()

# Hugging Face model references for Transformer library
models = dict(
    ROBERTA = "roberta-base",
    DISTILBERT_U = "distilbert-base-uncased",
    DISTILBERT_C = "distilbert-base-cased",
    DEBERTA_V2_XL = "microsoft/deberta-v2-xlarge")

# Logging date for w&b
from datetime import date
today = date.today()
log_date = today.strftime("%d-%m-%Y")

# Commented out IPython magic to ensure Python compatibility.
# RANDOM SEED FOR REPRODUCIBILITY
RANDOM_SEED = 42

# BATCH SIZE
# TRY 4, 8, 16, 32, 64, 128, 256. REDUCE IF OOM ERROR, HIGHER FOR TPUS
BATCH_SIZES = 2

# EPOCHS - TRANSFORMERS ARE TYPICALLY FINE-TUNED BETWEEN 1 AND 3 EPOCHS 
EPOCHS = 2

# WHICH PRE-TRAINED TRANSFORMER TO FINE-TUNE?
MODEL_CHECKPOINT = models['ROBERTA']

# SPECIFY THE WEIGHTS AND BIASES PROJECT NAME
# %env WANDB_PROJECT = 'P2D-NER-2021' 

# DETERMINE WHETHER TO SAVE THE MODEL IN THE 100GB OF FREE W&B STORAGE
# %env WANDB_LOG_MODEL = false

FEATURE_CLASS_LABELS = "/content/drive/MyDrive/paper-2-data-main/feature_class_labels.json"
DATA_FILE = '/content/drive/MyDrive/paper-2-data-main/cuad-v1-annotated.json'
TEMP_MODEL_OUTPUT_DIR = '/content/drive/MyDrive/paper-2-data-main/temp_model_output_dir'
SAVED_MODEL = f"/content/drive/MyDrive/paper-2-data-main/p2d-NER-Fine-Tune-Transformer-Final-{MODEL_CHECKPOINT}" # Change for notebook version

data_files = DATA_FILE
datasets = load_dataset('json', data_files=data_files, field='data')
print(datasets)

# Check the ner_tags to ensure that these are integers
datasets["train"].features["ner_tags"]

# Open the label list created in pre-processing corresponding to the ner_tag indices
with open(FEATURE_CLASS_LABELS, 'r') as f:
    label_list = json.load(f)

for n in range(len(label_list)):
    print(n, label_list[n])

# Check some random samples to ensure data loaded as expected:
def show_random_elements(dataset, num_examples=1):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):
            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])
    display(HTML(df.to_html()))

show_random_elements(datasets["train"], num_examples=3)

# Instantiate the tokenizer
#For RoBERTa-base, need to use RobertaTokenizerFast with add_prefix_space=True to use it with pretokenized inputs.

if MODEL_CHECKPOINT == models['ROBERTA']:
    tokenizer = RobertaTokenizerFast.from_pretrained(models["ROBERTA"], add_prefix_space=True)
else:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

def word_id_func(input_ids, print_labs=False):
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    
    word_ids = []
    i=0
    spec_toks = ['[CLS]', '[SEP]', '[PAD]']
    for t in tokens:
        if t in spec_toks:
            word_ids.append(-100)
            print(t, i) if print_labs else None
        elif t.startswith('▁'):
            i += 1
            word_ids.append(i)
            print(t, i) if print_labs else None
        else:
            word_ids.append(i)
            print(t, i) if print_labs else None
        print("Total:", i) if print_labs else None
    return word_ids

def tokenize_and_align_labels(examples, label_all_tokens=False):
    tokenized_inputs = tokenizer(examples["split_tokens"],
                                 truncation=True,
                                 is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

def tokenize_and_align_labels_deberta(examples, label_all_tokens=False):
    tokenized_inputs = tokenizer(examples["split_tokens"],
                                 truncation=True,
                                 is_split_into_words=True)
    labels = []
    word_ids_list = []
    for input_ids in tokenized_inputs["input_ids"]:
        wids = word_id_func(input_ids, print_labs=False)
        word_ids_list.append(wids)
    
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = word_ids_list[i]
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx == -100:
                label_ids.append(-100)
            #We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx-1])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx-1] if label_all_tokens else -100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# To apply this function on all the words and labels in our dataset,
# we just use the map method of our dataset object we created earlier.
# This will apply the function on all the elements of all the splits in dataset, so our training, 
# validation and testing data will be preprocessed in one single command.

# 🤗 Datasets warns you when it uses cached files, you can pass load_from_cache_file=False in the
# call to map to not use the cached files and force the preprocessing to be applied again.
if MODEL_CHECKPOINT == models['DEBERTA_V2_XL']:
    tokenize_and_align_labels = tokenize_and_align_labels_deberta

tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, load_from_cache_file=True)

model = AutoModelForTokenClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=len(label_list))

#Optimizer
learning_rate = 0.0000075
lr_max = learning_rate * BATCH_SIZES
weight_decay = 0.05

optimizer = AdamW(
    model.parameters(),
    lr=lr_max,
    weight_decay=weight_decay)

print("The maximum learning rate is: ",lr_max)

# Learning Rate Schedule
num_train_samples = len(datasets["train"])
warmup_ratio = 0.2 # Percentage of total steps to go from zero to max learning rate
num_cycles=0.8 # The cosine exponential rate

num_training_steps = num_train_samples*EPOCHS/BATCH_SIZES
num_warmup_steps = num_training_steps*warmup_ratio

lr_sched = get_cosine_schedule_with_warmup(optimizer=optimizer,
                                           num_warmup_steps=num_warmup_steps,
                                           num_training_steps = num_training_steps,
                                           num_cycles=num_cycles)

args = TrainingArguments(output_dir = TEMP_MODEL_OUTPUT_DIR,
                         learning_rate=lr_max,
                         per_device_train_batch_size=BATCH_SIZES,
                         num_train_epochs=EPOCHS,
                         weight_decay=weight_decay,
                         lr_scheduler_type = 'cosine',
                         warmup_ratio=warmup_ratio,
                         logging_strategy="epoch",
                         save_strategy="epoch",
                         seed=RANDOM_SEED,
                         report_to = 'wandb', # enable logging to W&B
                         run_name = MODEL_CHECKPOINT+"-"+log_date
                        )

data_collator = DataCollatorForTokenClassification(tokenizer)

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)]

    # Define the metric parameters
    overall_precision = precision_score(true_labels, true_predictions, zero_division=1)
    overall_recall = recall_score(true_labels, true_predictions, zero_division=1)
    overall_f1 = f1_score(true_labels, true_predictions, zero_division=1)
    overall_accuracy = accuracy_score(true_labels, true_predictions)
    
    # Return a dictionary with the calculated metrics
    return {
        "precision": overall_precision,
        "recall": overall_recall,
        "f1": overall_f1,
        "accuracy": overall_accuracy,}

# Define and instantiate the Trainer...
trainer = Trainer(
                model=model,
                args=args,
                train_dataset=tokenized_datasets["train"],
                data_collator=data_collator,
                tokenizer=tokenizer,
                optimizers=(optimizer, lr_sched)
                )

# Train
trainer.train()

# Finish Weighs & Biases logging for this run
wandb.finish()

# Save the model, good practice given the work required to train a model and  
# also can be used just for inference on new data
trainer.save_model(SAVED_MODEL)

"""# Final STEP :Test"""

from google.colab import drive
drive.mount('/content/drive')

pip install PyMuPDF

pip install datasets

pip install transformers

import os, re, math, random, json, string, csv

import pandas as pd
import numpy as np
from tqdm import tqdm
from IPython.display import display, HTML

import transformers
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer
from transformers import DataCollatorForTokenClassification, PreTrainedModel, RobertaTokenizerFast

from datasets import load_dataset, ClassLabel, Sequence 

import fitz # pip install PyMuPDF - PDF reader/parser

import spacy
from spacy.lang.en import English

from collections import defaultdict

# Resolve any conflicting libraries
os.environ['KMP_DUPLICATE_LIB_OK']='True'

# Hugging Face model references for Transformer library
models = dict(
    ROBERTA = "roberta-base", # Use for efficiency
    DEBERTA_V2_XL = "microsoft/deberta-v2-xlarge") # Use for accuracy

# RANDOM SEED FOR REPRODUCIBILITY
RANDOM_SEED = 42

# BATCH SIZE
# IDEALLY USE SAME BATCH SIZE FOR INFERENCE AS WAS USED FOR TRAINING
BATCH_SIZES = 2

# WHICH PRE-TRAINED TRANSFORMER TO FINE-TUNE?
MODEL_CHECKPOINT = models['ROBERTA']

FEATURE_CLASS_LABELS = "/content/drive/MyDrive/paper-2-data-main/feature_class_labels.json"
TEMP_MODEL_OUTPUT_DIR = '/content/drive/MyDrive/paper-2-data-main/temp_model_output_dir'
SAVED_MODEL = f"/content/drive/MyDrive/paper-2-data-main/p2d-NER-Fine-Tune-Transformer-Final-{MODEL_CHECKPOINT}" # Change for notebook version
TEST_FILE_PATH = "/content/drive/MyDrive/Test_Docs/"
TEST_DATA_FILE = '/content/drive/MyDrive/paper-2-data-main/test_data_file.json'
CSV_DATA_FILE = '/content/drive/MyDrive/paper-2-data-main/legal_agreement_data_file_last.csv'

# Walk through PDF files and create a dataframe with the names of the files, sorted alpha/num
pdf_files = []
for (dirpath, dirnames, filenames) in os.walk(TEST_FILE_PATH):
    pdf_files.extend(filenames)
# Remove any hidden files lurking in the directory
for i, f in enumerate(pdf_files):
    if f.startswith("."):
        pdf_files.pop(i)
print(f"Uploaded {len(pdf_files)} legal agreements from {TEST_FILE_PATH} folder: ", pdf_files)

# Text cleaning function for standard PDF parsing workflow
def pre_process_doc_common(text):
    text = text.replace("\n", " ")  # Simple replacement for "\n"   
    text = text.replace("\xa0", " ")  # Simple replacement for "\xa0"
    text = text.replace("\x0c", " ")  # Simple replacement for "\x0c"
    
    regex = "\ \.\ "
    subst = "."
    text = re.sub(regex, subst, text, 0)  # Get rid of multiple dots
        
    regex = "_"
    subst = " "
    text = re.sub(regex, subst, text, 0)  # Get rid of underscores
       
    regex = "--+"
    subst = " "
    text = re.sub(regex, subst, text, 0)   # Get rid of multiple dashes
        
    regex = "\*+"
    subst = "*"
    text = re.sub(regex, subst, text, 0)  # Get rid of multiple stars
        
    regex = "\ +"
    subst = " "
    text = re.sub(regex, subst, text, 0)  # Get rid of multiple whitespace
    
    text = text.strip()  #Strip leading and trailing whitespace
    return text

# Function to take in the file list, read each file, clean the text and return all agreements in a list
def text_data(test_dir, pdf_files, print_text=False, clean_text=True, max_len=3000):
    text_list = []
    for filename in tqdm(pdf_files):
        agreement = fitz.open(test_dir+filename)
        full_text = ""
        for page in agreement:
            full_text += page.getText('text')#+"\n"
        if print_text:
            print("Text before cleaning: \n", full_text)

        # Run text through cleansing function
        if clean_text:
            full_text = pre_process_doc_common(full_text)
        short_text = full_text[:max_len]
        len_text = len(short_text)

        if print_text:
            print("Text after cleaning: \n", short_text)

        text_list.append([filename, full_text, short_text, len_text])
        
    return text_list

# Run reading and cleaning functions on the list of PDF files in the testing folder
# Use a max_length which is expected to capture the rich text information at the beginning of the document
test_dir = TEST_FILE_PATH
data = text_data(test_dir, pdf_files, print_text=False, clean_text=True, max_len=1000)

# Create dataframe with text
columns = ['File_Name','Full_Text', 'Short_Text', 'Length_Of_Short_Text']
text_df = pd.DataFrame(data=data, columns=columns)

# Have a look at the unstructured data captured so far
text_df

# What does an agreement look like?
text_df['Full_Text'][0]

# We tokenize each agreement prior to bringing into the transformer model
# Create tokens using spaCy
nlp = English()
text_df['tokens'] = text_df['Short_Text'].apply(lambda x: nlp(x))

# Split tokens into a list ready for CSV
text_df['split_tokens'] = text_df['tokens'].apply(lambda x: [tok.text for tok in x])

# Create dummy NER tags for alignment purposes (a bit lazy, but convinient)
text_df['dummy_ner_tags'] = text_df['tokens'].apply(lambda x: [0 for tok in x])

# Serialise the data to JSON for archive
export_columns = ['split_tokens', 'dummy_ner_tags']
export_df = text_df[export_columns]
export_df.to_json(TEST_DATA_FILE, orient="table", index=False)
text_df = text_df.drop(['dummy_ner_tags'], axis=1)

# Re-import the serialized JSON data and create a dataset in the format needed for the transformer
data_files = TEST_DATA_FILE
datasets = load_dataset('json', data_files=data_files, field='data')
print(datasets)

# Open the label list created in pre-processing corresponding to the ner_tag indices
with open(FEATURE_CLASS_LABELS, 'r') as f:
    label_list = json.load(f)

for n in range(len(label_list)):
    print(n, label_list[n])

# Instantiate the tokenizer
#For RoBERTa-base, need to use RobertaTokenizerFast with add_prefix_space=True to use it with pretokenized inputs.

if MODEL_CHECKPOINT == models['ROBERTA']:
    tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base", add_prefix_space=True)
else:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

# Functions deal with split tokens and special tokens used in each Transformer model
def word_id_func(input_ids, print_labs=False):
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    
    word_ids = []
    i=0
    spec_toks = ['[CLS]', '[SEP]', '[PAD]']
    for t in tokens:
        if t in spec_toks:
            word_ids.append(-100)
            print(t, i) if print_labs else None
        elif t.startswith('▁'):
            i += 1
            word_ids.append(i)
            print(t, i) if print_labs else None
        else:
            word_ids.append(i)
            print(t, i) if print_labs else None
        print("Total:", i) if print_labs else None
    return word_ids

def tokenize_and_align_labels(examples, label_all_tokens=False):
    tokenized_inputs = tokenizer(examples["split_tokens"],
                                 truncation=True,
                                 is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["dummy_ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

def tokenize_and_align_labels_deberta(examples, label_all_tokens=False):
    tokenized_inputs = tokenizer(examples["split_tokens"],
                                 truncation=True,
                                 is_split_into_words=True)
    labels = []
    word_ids_list = []
    for input_ids in tokenized_inputs["input_ids"]:
        wids = word_id_func(input_ids, print_labs=False)
        word_ids_list.append(wids)
    
    for i, label in enumerate(examples["dummy_ner_tags"]):
        word_ids = word_ids_list[i]
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx == -100:
                label_ids.append(-100)
            #We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx-1])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx-1] if label_all_tokens else -100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# To apply this function on all the words and labels in our dataset,
# we just use the map method of our dataset object we created earlier.

# 🤗 Datasets warns you when it uses cached files, you can pass load_from_cache_file=False in the
# call to map to not use the cached files and force the preprocessing to be applied again.
if MODEL_CHECKPOINT == models['DEBERTA_V2_XL']:
    tokenize_and_align_labels = tokenize_and_align_labels_deberta

tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, load_from_cache_file=True)

# Load the model and instantiate
loaded_model = AutoModelForTokenClassification.from_pretrained(SAVED_MODEL)

args = TrainingArguments(output_dir = TEMP_MODEL_OUTPUT_DIR,
                         per_device_train_batch_size=BATCH_SIZES,
                         per_device_eval_batch_size=BATCH_SIZES,
                         seed=RANDOM_SEED
                        )

data_collator = DataCollatorForTokenClassification(tokenizer)

# Note instantiation currently takes a bit of time: https://github.com/huggingface/transformers/issues/9205
# Instantiate the predictor
pred_trainer = Trainer(
    loaded_model,
    args,
    data_collator=data_collator,
    tokenizer=tokenizer)

# Extract the predictions
predictions, labels, _ = pred_trainer.predict(tokenized_datasets["train"])
predictions = np.argmax(predictions, axis=2)
text_df['predictions'] = list(predictions)

# Remove ignored index (special tokens)
true_predictions = [
    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]
text_df['true_predictions'] = true_predictions

# Consolidate all the information into the DataFrame
def data_extract(tuple_list):
    de_list = []
    for tup in tuple_list:
        if tup[1] != 'O':
            de_list.append(tup)
    return de_list

text_df['check_pred'] = list(list(zip(a,b)) for a,b in zip(text_df['split_tokens'], text_df['true_predictions']))
text_df['data_tuples'] = text_df['check_pred'].apply(data_extract)

# Have a look at the label predictions
text_df.head()[['File_Name', 'true_predictions']]

text_df['data_tuples'][1]

new_text=text_df[['File_Name','Full_Text']].copy()
new_text.info()

#make a list of labals 
new_lbl=[]
lbl=label_list.copy()
lbl.pop()
for l in lbl:
  l=l[2:]  
  new_lbl.append(l)
new_lbl=list(set(new_lbl))

#function to extract labels 
def extract_agreement_parties(tuple_list):
    data_dict = defaultdict(list)
    for i, p in enumerate(tuple_list):
        if p[1] == "B-PARTY":
            temp_party=p[0]
            if i == (len(tuple_list)-1):
                data_dict["Parties"].append(temp_party)
            elif tuple_list[i+1][1] != "I-PARTY":
                data_dict["Parties"].append(temp_party)
        elif p[1] == "I-PARTY":
            temp_party = temp_party + " " + p[0]
            if i == (len(tuple_list)-1):
                data_dict["Parties"].append(temp_party)
            elif tuple_list[i+1][1] != "I-PARTY":
                data_dict["Parties"].append(temp_party)

    return list(dict.fromkeys(data_dict['Parties']))

new_text['agmt_parties'] = text_df['data_tuples'].apply(extract_agreement_parties)

new_lbl.remove('PARTY')
for l in new_lbl:
  def extract_agreement(tuple_list):
    temp=""
    for d in tuple_list:
      if d[1] =='B-'+l:
        temp=d[0]
      elif d[1]=='I-'+l:
        temp=temp+" "+d[0]
      else : 
        continue
    return temp
  new_text[l]=text_df['data_tuples'].apply(extract_agreement)





# Create a dataframe with just the information we want to keep and 
export_df = new_text
export_df = export_df.sort_values('File_Name', axis=0)

# Let's have a look
export_df.head()

# Example data
sample=1
print("File Name: \t\t",export_df.iloc[sample][0])
print("Agreement Name: \t",export_df.iloc[sample][1])
print("Agreement Date: \t",export_df.iloc[sample][2])
print("Effective Date: \t",export_df.iloc[sample][4])
print("Agreement Parties:")
for p in export_df.iloc[sample][3]:
    print("\t\t\t", p)

# Export to CSV file, upload to a database table or some other structured data format, we are done.
export_df.to_csv(CSV_DATA_FILE)